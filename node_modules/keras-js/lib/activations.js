'use strict';

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.softmax = softmax;
exports.softplus = softplus;
exports.softsign = softsign;
exports.relu = relu;
exports.tanh = tanh;
exports.sigmoid = sigmoid;
exports.hardSigmoid = hardSigmoid;
exports.linear = linear;

var _ndarrayOps = require('ndarray-ops');

var _ndarrayOps2 = _interopRequireDefault(_ndarrayOps);

var _cwise = require('cwise');

var _cwise2 = _interopRequireDefault(_cwise);

var _Tensor = require('./Tensor');

var _Tensor2 = _interopRequireDefault(_Tensor);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

/**
 * Softmax activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function softmax(x) {
  if (x.tensor.shape.length === 1) {
    var maxval = _ndarrayOps2.default.sup(x.tensor);
    _ndarrayOps2.default.subseq(x.tensor, maxval);
    _ndarrayOps2.default.expeq(x.tensor);
    var sum = _ndarrayOps2.default.sum(x.tensor);
    _ndarrayOps2.default.divseq(x.tensor, sum);
  } else if (x.tensor.shape.length === 2) {
    for (var i = 0; i < x.tensor.shape[0]; i++) {
      var _maxval = _ndarrayOps2.default.sup(x.tensor.pick(i, null));
      _ndarrayOps2.default.subseq(x.tensor.pick(i, null), _maxval);
      _ndarrayOps2.default.expeq(x.tensor.pick(i, null));
      var _sum = _ndarrayOps2.default.sum(x.tensor.pick(i, null));
      _ndarrayOps2.default.divseq(x.tensor.pick(i, null), _sum);
    }
  } else {
    throw new Error(`[activations.softmax] tensor shape ${x.tensor.shape} not supported.`);
  }
  return this;
}

var _softplus = (0, _cwise2.default)({
  args: ['array'],
  body: function body(_x) {
    _x = Math.log(Math.exp(_x) + 1);
  }
});

/**
 * Softplus activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function softplus(x) {
  _softplus(x.tensor);
  return this;
}

var _softsign = (0, _cwise2.default)({
  args: ['array'],
  body: function body(_x) {
    _x /= 1 + Math.abs(_x);
  }
});

/**
 * Softsign activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function softsign(x) {
  _softsign(x.tensor);
  return this;
}

/**
 * ReLU activation function. In-place operation.
 * @param {Tensor} x
 * @param {Number} alpha
 * @param {Number} maxValue
 * @returns {Tensor} `this`
 */
function relu(x) {
  var opts = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  var _opts$alpha = opts.alpha,
      alpha = _opts$alpha === undefined ? 0 : _opts$alpha,
      _opts$maxValue = opts.maxValue,
      maxValue = _opts$maxValue === undefined ? null : _opts$maxValue;

  var neg = void 0;
  if (alpha !== 0) {
    neg = new _Tensor2.default([], x.tensor.shape);
    _ndarrayOps2.default.mins(neg.tensor, x.tensor, 0);
    _ndarrayOps2.default.mulseq(neg.tensor, alpha);
  }
  _ndarrayOps2.default.maxseq(x.tensor, 0);
  if (maxValue) {
    _ndarrayOps2.default.minseq(x.tensor, maxValue);
  }
  if (neg) {
    _ndarrayOps2.default.addeq(x.tensor, neg.tensor);
  }
  return this;
}

var _tanh = (0, _cwise2.default)({
  args: ['array'],
  body: function body(_x) {
    _x = Math.tanh(_x);
  }
});

/**
 * Tanh activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function tanh(x) {
  _tanh(x.tensor);
  return this;
}

var _sigmoid = (0, _cwise2.default)({
  args: ['array'],
  body: function body(_x) {
    _x = 1 / (1 + Math.exp(-_x));
  }
});

/**
 * Sigmoid activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function sigmoid(x) {
  _sigmoid(x.tensor);
  return this;
}

// Reference hard sigmoid with slope and shift values from theano, see
// https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py
var _hardSigmoid = (0, _cwise2.default)({
  args: ['array'],
  body: function body(_x) {
    _x = _x * 0.2 + 0.5;
    if (_x <= 0) {
      _x = 0;
    } else if (_x >= 1) {
      _x = 1;
    }
  }
});

/**
 * Hard-sigmoid activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function hardSigmoid(x) {
  _hardSigmoid(x.tensor);
  return this;
}

/**
 * Linear activation function. In-place operation.
 * @param {Tensor} x
 * @returns {Tensor} `this`
 */
function linear(x) {
  return this;
}